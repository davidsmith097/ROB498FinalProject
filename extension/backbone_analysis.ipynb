{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52a7b49",
   "metadata": {},
   "source": [
    "# Important Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0c9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce446ed",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61477cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for batch, labels in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred = model(batch)\n",
    "            num_correct += (pred.argmax(dim=1) == labels).type(torch.float).sum().item()\n",
    "            total += len(labels)\n",
    "        accuracy = (num_correct / total) * 100\n",
    "        return accuracy\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        for batch, labels in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred = model(batch)\n",
    "            num_correct += (pred.argmax(dim=1) == labels).type(torch.float).sum().item()\n",
    "            total += len(labels)\n",
    "        accuracy = (num_correct / total) * 100\n",
    "        return accuracy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device):\n",
    "    NUM_EPOCHS = 3\n",
    "    learning_rate = 0.0001\n",
    "    adam_beta1 = 0.9\n",
    "    adam_beta2 = 0.999\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, betas=(adam_beta1, adam_beta2))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    epoch_loss = []\n",
    "    train_loss = []\n",
    "    validaction_acc = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Epoch %d\" % epoch)\n",
    "        for step_num, (batch, labels) in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            pred = model(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step_num + 1) % 13 == 0:\n",
    "                # Perform validation and store accuracy\n",
    "                validation_accuracy = validate_model(model=model, val_loader=val_loader, device=device)\n",
    "                validaction_acc.append(validation_accuracy)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "\n",
    "        # Track average loss for each epoch\n",
    "        epoch_loss.append(sum(train_loss) / len(train_loss))\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    return epoch_loss, train_loss, validaction_acc, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebec10",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71af0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Create Data Augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomChoice([\n",
    "        transforms.RandomApply([\n",
    "            transforms.ElasticTransform(alpha=40.0, sigma=8.0)\n",
    "        ], p=0.2),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomAffine(degrees=0, shear=20, fill=255)\n",
    "        ], p=0.2),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomAffine(degrees=0, scale=(0.8, 1.2), fill=255)\n",
    "        ], p=0.2),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomHorizontalFlip(p=1.0)\n",
    "        ], p=0.2),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomVerticalFlip(p=1.0)\n",
    "        ], p=0.2),\n",
    "    ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Training, Validation, and Testing Images\n",
    "LABELS = [\"Apple Scab\", \"Apple Black Rot\", \"Apple Cedar Rust\", \"Apple Healthy\", \"Blueberry Healthy\", \"Cherry Healthy\", \"Cherry Powdery Mildew\", \"Corn Cercospora Leaf Spot\", \"Corn Common Rust\", \"Corn Healthy\", \"Corn Northern Leaf Blight\", \"Grape Black Rot\", \"Grape Black Measles\", \"Grape Healthy\", \"Grape Isariopsis Leaf Spot\", \"Orange Haunglonbing\",\n",
    "          \"Peach Bacterial Spot\", \"Peach Healthy\", \"Bell Pepper Bacterial Spot\", \"Bell Pepper Healthy\", \"Potato Early Blight\", \"Potato Healthy\", \"Potato Late Blight\", \"Raspberry Healthy\", \"Soybean Healthy\", \"Squash Powdery Mildew\", \"Strawberry Healthy\", \"Strawberry Leaf Scorch\", \"Tomato Bacterial Spot\", \"Tomato Early Blight\", \"Tomato Healthy\",\n",
    "          \"Tomato Late Blight\", \"Tomato Leaf Mold\", \"Tomato Septoria Leaf Spot\", \"Tomato Spider Mites\", \"Tomato Target Spot\", \"Tomato Mosaic Virus\", \"Tomato Yellow Leaf Curl Virus\"]\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Running on {DEVICE}\")\n",
    "folder_path = \"PlantVillage\"\n",
    "\n",
    "train_set = ImageFolder(root=folder_path + \"\\Training\", transform=data_transforms)\n",
    "val_set = ImageFolder(root=folder_path + \"\\Validation\", transform=transforms.ToTensor())\n",
    "test_set = ImageFolder(root=folder_path + \"\\Testing\", transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e38189",
   "metadata": {},
   "source": [
    "# Load Pretrained Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d6dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "# Load all backbones\n",
    "resnet101 = torchvision.models.resnet101(weights=torchvision.models.ResNet101_Weights.DEFAULT).to(device=DEVICE)\n",
    "resnet50 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT).to(device=DEVICE)\n",
    "efficientnetb0 = torchvision.models.efficientnet_b0(weights=torchvision.models.EfficientNet_B0_Weights.DEFAULT).to(device=DEVICE)\n",
    "efficientnetb1 = torchvision.models.efficientnet_b1(weights=torchvision.models.EfficientNet_B1_Weights.DEFAULT).to(device=DEVICE)\n",
    "efficientnetb2 = torchvision.models.efficientnet_b2(weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT).to(device=DEVICE)\n",
    "efficientnetb3 = torchvision.models.efficientnet_b3(weights=torchvision.models.EfficientNet_B3_Weights.DEFAULT).to(device=DEVICE)\n",
    "densenet121 = torchvision.models.densenet121(weights=torchvision.models.DenseNet121_Weights.DEFAULT).to(device=DEVICE)\n",
    "vgg16_bn = torchvision.models.vgg16_bn(weights=torchvision.models.VGG16_BN_Weights.DEFAULT).to(device=DEVICE)\n",
    "\n",
    "# Remove last layer for each backbone\n",
    "resnet101.fc = nn.Identity()\n",
    "resnet50.fc = nn.Identity()\n",
    "efficientnetb0.classifier[1] = nn.Identity()\n",
    "efficientnetb1.classifier[1] = nn.Identity()\n",
    "efficientnetb2.classifier[1] = nn.Identity()\n",
    "efficientnetb3.classifier[1] = nn.Identity()\n",
    "densenet121.classifier = nn.Identity()\n",
    "vgg16_bn.classifier[6] = nn.Identity()\n",
    "\n",
    "# Prep list of model and output size\n",
    "backbones = [(\"resnet101\", resnet101, 2048), (\"resnet50\", resnet50, 2048), (\"efficientnetb0\", efficientnetb0, 1280), (\"efficientnetb1\", efficientnetb1, 1280), (\"efficientnetb2\", efficientnetb2, 1280), (\"efficientnetb3\", efficientnetb3, 1536), (\"densenet121\", densenet121, 1024), (\"vgg16_bn\", vgg16_bn, 32768)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded52b79",
   "metadata": {},
   "source": [
    "# Benchmark Each Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6ea1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet101\n",
      "Epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(backbone, nn\u001b[38;5;241m.\u001b[39mLinear(backbone_output_size, NUM_CLASSES, device\u001b[38;5;241m=\u001b[39mDEVICE), nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 10\u001b[0m epoch_loss, train_loss, val_acc, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Test model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m epoch)\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step_num, (batch, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     50\u001b[0m         batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     51\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 38\n",
    "benchmarks = {}\n",
    "for name, backbone, backbone_output_size in backbones:\n",
    "    print(name)\n",
    "    # Freeze backbone parameters\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Create model\n",
    "    model = nn.Sequential(backbone, nn.Linear(backbone_output_size, NUM_CLASSES, device=DEVICE), nn.Softmax(dim=1)).to(DEVICE)\n",
    "    epoch_loss, train_loss, val_acc, train_time = train_model(model, train_loader, val_loader, DEVICE)\n",
    "    # Test model\n",
    "    model.eval()\n",
    "    test_acc = test_model(model, test_loader, DEVICE)\n",
    "    benchmarks[name] = (epoch_loss, train_loss, val_acc, test_acc, train_time)\n",
    "\n",
    "# Save benchmark information\n",
    "with open(\"benchmarks.pickle\", \"wbx\") as file:\n",
    "    pickle.dump(benchmarks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c446cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark information\n",
    "with open(\"benchmarks.pickle\", \"rb\") as file:\n",
    "    loaded_benchmarks = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
